{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Apache Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accessLog = sc.textFile('/user/pradeep.s.rajpoot_gmail/access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "row_pattern = re.compile('^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)(.*)\" (\\d{3}) (\\S+)')\n",
    "def parseLogLine(x):\n",
    "    m = row_pattern.match(x)\n",
    "    if m:\n",
    "        tokens = m.group(4).split(':')\n",
    "        tFrame = tokens[0] + ':' + tokens[1]\n",
    " \n",
    "        #return { \"host\" : m.group(1), \"timeStamp\" : m.group(4), \"url\" : m.group(6), \"httpCode\" : int(m.group(8)) }\n",
    "        return Row(host=m.group(1), timeStamp=m.group(4), timeFrame=tFrame, url=m.group(6), httpCode=int(m.group(8)))\n",
    "    else:\n",
    "        #return {}\n",
    "        return Row(host=None, timeStamp=None, timeFrame = None, url=None, httpCode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_df = accessLog.map(parseLogLine).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(host=u'in24.inetnebr.com', httpCode=200, timeFrame=u'01/Aug/1995:00', timeStamp=u'01/Aug/1995:00:00:01 -0400', url=u'/shuttle/missions/sts-68/news/sts-68-mcc-05.txt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- httpCode: long (nullable = true)\n",
      " |-- timeFrame: string (nullable = true)\n",
      " |-- timeStamp: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/user/pradeep.s.rajpoot_gmail/access MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839',\n",
       " u'uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessLog.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "\n",
    "Write spark code( using RDD) to find out top 10 requested URLs along with count of number of times they have been requested (This information will help company to find out most popular pages and how frequently they are accessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               URL     Count\n",
      "\n",
      "                        /images/NASA-logosmall.gif     97410\n",
      "                         /images/KSC-logosmall.gif     75337\n",
      "                      /images/MOSAIC-logosmall.gif     67448\n",
      "                         /images/USA-logosmall.gif     67068\n",
      "                       /images/WORLD-logosmall.gif     66444\n",
      "                        /images/ksclogo-medium.gif     62778\n",
      "                                         /ksc.html     43687\n",
      "           /history/apollo/images/apollo-logo1.gif     37826\n",
      "                           /images/launch-logo.gif     35138\n",
      "                                                 /     30347\n"
     ]
    }
   ],
   "source": [
    "urls = log_df.rdd.filter(lambda x: x['url'] != None).map(lambda x : x.url).countByValue()\n",
    "#urls = accessLog.map(lambda x : x.split(' ')[6]).countByValue()\n",
    "sorted_urls = sorted([ (v, k) for (k,v) in urls.items()], reverse=True)\n",
    "print ('URL'.rjust(50) + 'Count'.rjust(10))\n",
    "print\n",
    "for x in sorted_urls[0:10]:\n",
    "    print (str(x[1]).rjust(50) + str(x[0]).rjust(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hosts = accessLog.map(lambda x : x.split(' ')[0]).countByValue()\n",
    "#sorted_hosts = sorted([ (v, k) for (k,v) in hosts.items()], reverse=True)\n",
    "#print ('IP'.rjust(50) + 'Count'.rjust(10))\n",
    "#print\n",
    "#for x in sorted_hosts[0:5]:\n",
    "#    print (str(x[1]).rjust(50) + str(x[0]).rjust(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Write spark code to find out top 5 hosts / IP making the request along with count (This information will help company to find out locations where website is popular or to figure out potential DDoS attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                host|count|\n",
      "+--------------------+-----+\n",
      "|  edams.ksc.nasa.gov| 6530|\n",
      "|piweba4y.prodigy.com| 4846|\n",
      "|        163.206.89.4| 4791|\n",
      "|piweba5y.prodigy.com| 4607|\n",
      "|piweba3y.prodigy.com| 4416|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.groupBy('host').count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Write spark code to find out top 5 time frame for high traffic (which day of the week or hour of the day receives peak traffic, this information will help company to manage resources for handling peak traffic load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|     timeFrame|count|\n",
      "+--------------+-----+\n",
      "|31/Aug/1995:11| 6319|\n",
      "|31/Aug/1995:10| 6283|\n",
      "|31/Aug/1995:13| 5948|\n",
      "|30/Aug/1995:15| 5919|\n",
      "|31/Aug/1995:09| 5627|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.groupBy('timeFrame').count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Write spark code to find out 5 time frames of least traffic (which day of the week or hour of the day receives least traffic, this information will help company to do production deployment in that time frame so that less number of users will be affected if some thing goes wrong during deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|     timeFrame|count|\n",
      "+--------------+-----+\n",
      "|03/Aug/1995:04|   16|\n",
      "|03/Aug/1995:09|   22|\n",
      "|03/Aug/1995:05|   43|\n",
      "|03/Aug/1995:10|   57|\n",
      "|03/Aug/1995:07|   58|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.groupBy('timeFrame').count().orderBy('count', ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Write spark code to find out unique HTTP codes returned by the server along with count (this information is helpful for devops team to find out how many requests are failing so that appropriate action can be taken to fix the issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|httpCode|  count|\n",
      "+--------+-------+\n",
      "|     200|1398988|\n",
      "|     304| 134146|\n",
      "|     302|  26444|\n",
      "|     404|  10056|\n",
      "|     403|    171|\n",
      "|     501|     27|\n",
      "|     500|      3|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.filter(log_df.httpCode.isNotNull()).groupBy('httpCode').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
